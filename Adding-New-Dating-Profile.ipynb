{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding a New Dating Profile\n",
    "Using Classification or Clustering for a New Dating Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "pd.set_option('display.max_colwidth', 500)\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import _pickle as pickle\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.cluster import AgglomerativeClustering\r\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score, davies_bouldin_score\r\n",
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import confusion_matrix, classification_report\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bios</th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Passionate analyst. Explorer. Hipster-friendly problem solver. Freelance music geek. Social media advocate. Reader.</td>\n      <td>5</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Twitter fanatic. Devoted web fanatic. Zombie evangelist. Travel aficionado. Bacon lover.</td>\n      <td>5</td>\n      <td>7</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Total alcohol practitioner. Social media buff. Evil beer expert. Devoted analyst. Problem solver. Student.</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Extreme twitter advocate. Hardcore internet junkie. Entrepreneur. Friend of animals everywhere.</td>\n      <td>4</td>\n      <td>6</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Problem solver. Devoted introvert. Food geek. Avid writer. Thinker. Troublemaker. Friend of animals everywhere.</td>\n      <td>7</td>\n      <td>2</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                  Bios  \\\n0  Passionate analyst. Explorer. Hipster-friendly problem solver. Freelance music geek. Social media advocate. Reader.   \n1                             Twitter fanatic. Devoted web fanatic. Zombie evangelist. Travel aficionado. Bacon lover.   \n2           Total alcohol practitioner. Social media buff. Evil beer expert. Devoted analyst. Problem solver. Student.   \n3                      Extreme twitter advocate. Hardcore internet junkie. Entrepreneur. Friend of animals everywhere.   \n4      Problem solver. Devoted introvert. Food geek. Avid writer. Thinker. Troublemaker. Friend of animals everywhere.   \n\n   Loation  Qualification  Specialisation  \n0        5              3               1  \n1        5              7               3  \n2        2              0               2  \n3        4              6               6  \n4        7              2               8  "
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in the cleaned DF\r\n",
    "with open(\"mechanic_profiles.pkl\",'rb') as fp:\r\n",
    "    raw_df = pickle.load(fp)\r\n",
    "\r\n",
    "# Viewing the DF    \r\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Clustered Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bios</th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n      <th>Cluster #</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6160</th>\n      <td>Lifelong zombie junkie. Friendly travel buff. Coffeeaholic. Internet enthusiast.</td>\n      <td>7.0</td>\n      <td>5.0</td>\n      <td>3.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6161</th>\n      <td>Total introvert. Tv specialist. Pop culture ninja. Web lover. Subtly charming twitter advocate. Explorer.</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6162</th>\n      <td>Friendly zombie specialist. Avid bacon expert. Tv junkie. Alcohol guru. Food aficionado.</td>\n      <td>6.0</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6163</th>\n      <td>Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6164</th>\n      <td>Lifelong travel expert. Evil gamer. Infuriatingly humble introvert. Devoted web junkie. Typical internet practitioner. Passionate alcohol buff.</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                                                 Bios  \\\n6160                                                                 Lifelong zombie junkie. Friendly travel buff. Coffeeaholic. Internet enthusiast.   \n6161                                        Total introvert. Tv specialist. Pop culture ninja. Web lover. Subtly charming twitter advocate. Explorer.   \n6162                                                         Friendly zombie specialist. Avid bacon expert. Tv junkie. Alcohol guru. Food aficionado.   \n6163                                              Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.   \n6164  Lifelong travel expert. Evil gamer. Infuriatingly humble introvert. Devoted web junkie. Typical internet practitioner. Passionate alcohol buff.   \n\n      Loation  Qualification  Specialisation  Cluster #  \n6160      7.0            5.0             3.0          1  \n6161      3.0            1.0             8.0          1  \n6162      6.0            7.0             6.0          2  \n6163      7.0            7.0             3.0          4  \n6164      9.0            1.0             0.0          3  "
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading in the clustered DF\n",
    "with open(\"clustered_profiles.pkl\",'rb') as fp:\n",
    "    cluster_df = pickle.load(fp)\n",
    "\n",
    "# Viewing the DF    \n",
    "cluster_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the New Profile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter new profile information...\n",
      "\n",
      "Example Bio:\n",
      "Bacon enthusiast. Falls down a lot. Freelance social media fan. Infuriatingly humble introvert.\n"
     ]
    }
   ],
   "source": [
    "# Instantiating a new DF row to append later\n",
    "new_profile = pd.DataFrame(columns=raw_df.columns)\n",
    "\n",
    "# Adding random values for new data\n",
    "for i in new_profile.columns[1:]:\n",
    "    new_profile[i] = np.random.randint(0,10,1)\n",
    "\n",
    "# Printing an user interface for inputting new values\n",
    "print(\"Enter new profile information...\\n\\nExample Bio:\\nBacon enthusiast. Falls down a lot. Freelance social media fan. Infuriatingly humble introvert.\")\n",
    "\n",
    "# Asking for new profile data\n",
    "new_profile['Bios'] = input(\"Enter a Bio for yourself: \")\n",
    "\n",
    "# Indexing that new profile data\n",
    "new_profile.index = [raw_df.index[-1] + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bios</th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6165</th>\n      <td>i need some help with heavy vechicle</td>\n      <td>7</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                      Bios  Loation  Qualification  \\\n6165  i need some help with heavy vechicle        7              7   \n\n      Specialisation  \n6165               7  "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Approaches\n",
    "1. Cluster all the profiles again with the new profile\n",
    "\n",
    "2. Classify the new profile with a classification model trained on our previously clustered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the New Profile Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the new data\n",
    "new_cluster = raw_df.append(new_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scaling the categories then replacing the old values\n",
    "df = new_cluster[['Bios']].join(pd.DataFrame(scaler.fit_transform(new_cluster.drop('Bios', axis=1)), columns=new_cluster.columns[1:], index=new_cluster.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fitting the vectorizer to the Bios\n",
    "x = vectorizer.fit_transform(df['Bios'])\n",
    "\n",
    "# Creating a new DF that contains the vectorized words\n",
    "df_wrds = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Concating the words DF with the original DF\n",
    "new_df = pd.concat([df, df_wrds], axis=1)\n",
    "\n",
    "# Dropping the Bios because it is no longer needed in place of vectorization\n",
    "new_df.drop('Bios', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9896490759603953"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiating PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fitting and Transforming the DF\n",
    "df_pca = pca.fit_transform(new_df)\n",
    "\n",
    "# Finding the exact number of features that explain at least 99% of the variance in the dataset\n",
    "total_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "n_over_99 = len(total_explained_variance[total_explained_variance>=.99])\n",
    "n_to_reach_99 = new_df.shape[1] - n_over_99\n",
    "\n",
    "# Reducing the dataset to the number of features determined before\n",
    "pca = PCA(n_components=n_to_reach_99)\n",
    "\n",
    "# Fitting and transforming the dataset to the stated number of features\n",
    "df_pca = pca.fit_transform(new_df)\n",
    "\n",
    "# Seeing the variance ratio that still remains after the dataset has been reduced\n",
    "pca.explained_variance_ratio_.cumsum()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Hierarchical Agglomerative Clustering\n",
    "- First finding the optimum number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-e5c1199d955c>:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm(cluster_cnt):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa87b5d48cc42c080d00cd2d9ad4ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=18.0), HTML(value='')))"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting the amount of clusters to test out\r\n",
    "cluster_cnt = [i for i in range(2, 20, 1)]\r\n",
    "\r\n",
    "# Establishing empty lists to store the scores for the evaluation metrics\r\n",
    "ch_scores = []\r\n",
    "\r\n",
    "s_scores = []\r\n",
    "\r\n",
    "db_scores = []\r\n",
    "\r\n",
    "# Looping through different iterations for the number of clusters\r\n",
    "for i in tqdm(cluster_cnt):\r\n",
    "    \r\n",
    "    # Clustering with different number of clusters\r\n",
    "    hac = AgglomerativeClustering(n_clusters=i)\r\n",
    "    \r\n",
    "    hac.fit(df_pca)\r\n",
    "    \r\n",
    "    cluster_assignments = hac.labels_\r\n",
    "    \r\n",
    "    # Appending the scores to the empty lists\r\n",
    "    ch_scores.append(calinski_harabasz_score(df_pca, cluster_assignments))\r\n",
    "    \r\n",
    "    s_scores.append(silhouette_score(df_pca, cluster_assignments))\r\n",
    "    \r\n",
    "    db_scores.append(davies_bouldin_score(df_pca, cluster_assignments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function to Evaluate the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_eval(y, x):\n",
    "    \"\"\"\n",
    "    Prints the scores of a set evaluation metric. Prints out the max and min values of the evaluation scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating a DataFrame for returning the max and min scores for each cluster\n",
    "    df = pd.DataFrame(columns=['Cluster Score'], index=[i for i in range(2, len(y)+2)])\n",
    "    df['Cluster Score'] = y\n",
    "    \n",
    "    print('Max Value:\\nCluster #', df[df['Cluster Score']==df['Cluster Score'].max()])\n",
    "    print('\\nMin Value:\\nCluster #', df[df['Cluster Score']==df['Cluster Score'].min()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Calinski-Harabasz Score (find max score):\n",
      "Max Value:\n",
      "Cluster #    Cluster Score\n",
      "3     143.675265\n",
      "\n",
      "Min Value:\n",
      "Cluster #     Cluster Score\n",
      "19      78.497954\n",
      "\n",
      "The Silhouette Coefficient Score (find max score):\n",
      "Max Value:\n",
      "Cluster #    Cluster Score\n",
      "2       0.069948\n",
      "\n",
      "Min Value:\n",
      "Cluster #    Cluster Score\n",
      "3       0.027911\n",
      "\n",
      "The Davies-Bouldin Score (find minimum score):\n",
      "Max Value:\n",
      "Cluster #    Cluster Score\n",
      "3       4.520621\n",
      "\n",
      "Min Value:\n",
      "Cluster #    Cluster Score\n",
      "2       3.263941\n"
     ]
    }
   ],
   "source": [
    "print(\"The Calinski-Harabasz Score (find max score):\")\n",
    "cluster_eval(ch_scores, cluster_cnt)\n",
    "\n",
    "print(\"\\nThe Silhouette Coefficient Score (find max score):\")\n",
    "cluster_eval(s_scores, cluster_cnt)\n",
    "\n",
    "print(\"\\nThe Davies-Bouldin Score (find minimum score):\")\n",
    "cluster_eval(db_scores, cluster_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running HAC\n",
    "Again but with the optimum cluster count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating HAC\n",
    "hac = AgglomerativeClustering(n_clusters=12)\n",
    "\n",
    "# Fitting\n",
    "hac.fit(df_pca)\n",
    "\n",
    "# Getting cluster assignments\n",
    "cluster_assignments = hac.labels_\n",
    "\n",
    "# Unscaling the categories then replacing the scaled values\n",
    "df = df[['Bios']].join(pd.DataFrame(scaler.inverse_transform(df.drop('Bios', axis=1)), columns=df.columns[1:], index=df.index))\n",
    "\n",
    "# Assigning the clusters to each profile\n",
    "df['Cluster #'] = cluster_assignments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Exact Cluster for our New Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the Cluster # for the new profile\n",
    "profile_cluster = df.loc[new_profile.index]['Cluster #'].values[0]\n",
    "\n",
    "# Using the Cluster # to narrow down the DF\n",
    "profile_df = df[df['Cluster #']==profile_cluster].drop('Cluster #', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the Selected Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the vectorizer to the Bios\n",
    "cluster_x = vectorizer.fit_transform(profile_df['Bios'])\n",
    "\n",
    "# Creating a new DF that contains the vectorized words\n",
    "cluster_v = pd.DataFrame(cluster_x.toarray(), index=profile_df.index, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Joining the Vectorized DF to the previous DF\n",
    "profile_df = profile_df.join(cluster_v).drop('Bios', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Correlation for Top 10 Similar Profiles to the New Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasnposing the DF so that we are correlating with the index(users) and finding the correlation\n",
    "corr = profile_df.T.corr()\n",
    "\n",
    "# Finding the Top 10 similar or correlated users to the new user\n",
    "user_n = new_profile.index[0]\n",
    "\n",
    "# Creating a DF with the Top 10 most similar profiles\n",
    "top_10_sim = corr[[user_n]].sort_values(by=[user_n],axis=0, ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Top 10 Profiles most likely to Match with the New Profile\n",
    "(Sorted by descending similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bios</th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3149</th>\n      <td>Freelance bacon expert. Music enthusiast. Unapologetic internetaholic.</td>\n      <td>8</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2231</th>\n      <td>Writer. Devoted travel advocate. Zombie fan. Internet evangelist.</td>\n      <td>7</td>\n      <td>9</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3179</th>\n      <td>Freelance bacon expert. Music enthusiast. Unapologetic internetaholic.</td>\n      <td>6</td>\n      <td>6</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2189</th>\n      <td>Introvert. Unapologetic bacon trailblazer. Devoted internet expert. Award-winning music advocate.</td>\n      <td>7</td>\n      <td>9</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3695</th>\n      <td>Freelance analyst. Troublemaker. General tv aficionado. Lifelong coffee guru. Avid communicator.</td>\n      <td>7</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1938</th>\n      <td>Explorer. Devoted tv enthusiast. Student. Award-winning alcohol nerd. Evil web fan. Internet lover.</td>\n      <td>8</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2857</th>\n      <td>Troublemaker. Explorer. Freelance travel guru. Coffee buff. Internet maven. Alcohol expert.</td>\n      <td>8</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>6016</th>\n      <td>Evil creator. Travel maven. Award-winning internet expert. Gamer. Troublemaker. Incurable student. Entrepreneur. Webaholic.</td>\n      <td>9</td>\n      <td>7</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2815</th>\n      <td>Thinker. Professional reader. Webaholic. Award-winning bacon advocate. Hardcore food ninja. Freelance travel fan.</td>\n      <td>8</td>\n      <td>8</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5554</th>\n      <td>Award-winning music evangelist. Communicator. Subtly charming troublemaker. Food guru. Infuriatingly humble web ninja.</td>\n      <td>9</td>\n      <td>8</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                                             Bios  \\\n3149                                                       Freelance bacon expert. Music enthusiast. Unapologetic internetaholic.   \n2231                                                            Writer. Devoted travel advocate. Zombie fan. Internet evangelist.   \n3179                                                       Freelance bacon expert. Music enthusiast. Unapologetic internetaholic.   \n2189                            Introvert. Unapologetic bacon trailblazer. Devoted internet expert. Award-winning music advocate.   \n3695                             Freelance analyst. Troublemaker. General tv aficionado. Lifelong coffee guru. Avid communicator.   \n1938                          Explorer. Devoted tv enthusiast. Student. Award-winning alcohol nerd. Evil web fan. Internet lover.   \n2857                                  Troublemaker. Explorer. Freelance travel guru. Coffee buff. Internet maven. Alcohol expert.   \n6016  Evil creator. Travel maven. Award-winning internet expert. Gamer. Troublemaker. Incurable student. Entrepreneur. Webaholic.   \n2815            Thinker. Professional reader. Webaholic. Award-winning bacon advocate. Hardcore food ninja. Freelance travel fan.   \n5554       Award-winning music evangelist. Communicator. Subtly charming troublemaker. Food guru. Infuriatingly humble web ninja.   \n\n      Loation  Qualification  Specialisation  \n3149        8              8               9  \n2231        7              9               7  \n3179        6              6               7  \n2189        7              9               9  \n3695        7              8               9  \n1938        8              8               9  \n2857        8              7               7  \n6016        9              7               9  \n2815        8              8               7  \n5554        9              8               7  "
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.loc[top_10_sim.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of the New Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Different Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing 3 models\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the split variables\n",
    "X = cluster_df.drop([\"Cluster #\"], 1)\n",
    "y = cluster_df['Cluster #']\n",
    "\n",
    "## Vectorizing\n",
    "# Instantiating the Vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fitting the vectorizer to the Bios\n",
    "x = vectorizer.fit_transform(X['Bios'])\n",
    "\n",
    "# Creating a new DF that contains the vectorized words\n",
    "df_wrds = pd.DataFrame(x.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Concating the words DF with the original DF\n",
    "X = pd.concat([X, df_wrds], axis=1)\n",
    "\n",
    "# Dropping the Bios because it is no longer needed in place of vectorization\n",
    "X.drop(['Bios'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scaling the Data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the New Profile Data\n",
    "For Vectorization purposes, the new profile will have to be able to fit into trained data (has to have the same columns).\n",
    "\n",
    "Two Options:\n",
    "1. __Vectorized the New Profile data with the vectorizer fitted to the dataset as to not include potentially new vocabulary. _(Keeps dimensionality the same)___\n",
    "2. Vectorized the New Profile data with a new vectorizer fitted to it in order to include new vocabulary. _(Increases dimensionality with every new piece of data)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing the new data\n",
    "vect_new_prof = vectorizer.transform(new_profile['Bios'])\n",
    "\n",
    "# Quick DF of the vectorized words\n",
    "new_vect_w = pd.DataFrame(vect_new_prof.toarray(), columns=vectorizer.get_feature_names(), index=new_profile.index)\n",
    "\n",
    "# Concatenating the DFs for the new profile data\n",
    "new_vect_prof = pd.concat([new_profile, new_vect_w], 1).drop('Bios', 1)\n",
    "\n",
    "# Scaling the new profile data\n",
    "new_vect_prof = pd.DataFrame(scaler.transform(new_vect_prof), columns=new_vect_prof.columns, index=new_vect_prof.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n      <th>advocate</th>\n      <th>aficionado</th>\n      <th>alcohol</th>\n      <th>alcoholaholic</th>\n      <th>amateur</th>\n      <th>analyst</th>\n      <th>animals</th>\n      <th>...</th>\n      <th>unable</th>\n      <th>unapologetic</th>\n      <th>wannabe</th>\n      <th>web</th>\n      <th>webaholic</th>\n      <th>winning</th>\n      <th>with</th>\n      <th>writer</th>\n      <th>zombie</th>\n      <th>zombieaholic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6165</th>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 113 columns</p>\n</div>",
      "text/plain": "       Loation  Qualification  Specialisation  advocate  aficionado  alcohol  \\\n6165  0.777778       0.777778        0.777778       0.0         0.0      0.0   \n\n      alcoholaholic  amateur  analyst  animals  ...  unable  unapologetic  \\\n6165            0.0      0.0      0.0      0.0  ...     0.0           0.0   \n\n      wannabe  web  webaholic  winning  with  writer  zombie  zombieaholic  \n6165      0.0  0.0        0.0      0.0   1.0     0.0     0.0           0.0  \n\n[1 rows x 113 columns]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vect_prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Model\n",
    "- Dummy (Baseline Model)\n",
    "- KNN\n",
    "- SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy\n",
    "dummy = DummyClassifier(strategy='stratified')\n",
    "\n",
    "# KNN\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# SVM\n",
    "svm = SVC()\n",
    "\n",
    "# List of models\n",
    "models = [dummy, knn, svm]\n",
    "\n",
    "# List of model names\n",
    "names = ['Dummy', 'KNN', 'SVM']\n",
    "\n",
    "# Zipping the lists\n",
    "classifiers = dict(zip(names, models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with an imbalanced dataset _(because each cluster is not guaranteed to have the same amount of profiles)_, we will resort to using the __Macro Avg__ and __F1 Score__ for evaluating the performances of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dummy (Macro Avg - F1 Score):\n",
      "0.08882137923883537\n",
      "\n",
      "KNN (Macro Avg - F1 Score):\n",
      "1.0\n",
      "\n",
      "SVM (Macro Avg - F1 Score):\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Dictionary containing the model names and their scores\n",
    "models_f1 = {}\n",
    "\n",
    "# Looping through each model's predictions and getting their classification reports\n",
    "for name, model in classifiers.items():\n",
    "    # Fitting the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print('\\n'+ name + ' (Macro Avg - F1 Score):')\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(y_test, model.predict(X_test), output_dict=True)\n",
    "    f1 = report['macro avg']['f1-score']\n",
    "    \n",
    "    # Assigning to the Dictionary\n",
    "    models_f1[name] = f1\n",
    "    \n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with the Best Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(max(models_f1, key=models_f1.get), 'Score:', max(models_f1.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Best Model to Classify the New Profile\n",
    "_(Optional: Tune the model with GridSearch)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([4], dtype=int64)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model\n",
    "svm.fit(X, y)\n",
    "\n",
    "# Predicting the New Profile data by determining which Cluster it would belong to\n",
    "designated_cluster = svm.predict(new_vect_prof)\n",
    "\n",
    "designated_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF containing the Profiles of the Designated Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bios</th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n      <th>Cluster #</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>Certified web evangelist. Proud bacon trailblazer. Travel aficionado. Alcohol scholar.</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Hipster-friendly musicaholic. Wannabe tv fanatic. Certified gamer. Amateur coffee specialist.</td>\n      <td>9.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Total entrepreneur. Proud web fanatic. Typical beer scholar. Student. Lifelong explorer. Tv maven.</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Music fan. Beer geek. Web lover. Falls down a lot. Coffee nerd. Travel junkie.</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Falls down a lot. Typical beer guru. Creator. Subtly charming alcohol enthusiast. Incurable tv buff. Reader.</td>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6133</th>\n      <td>Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>6.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6143</th>\n      <td>Writer. Total coffee scholar. Travel lover. Thinker. Troublemaker. Hardcore bacon practitioner.</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6148</th>\n      <td>Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6158</th>\n      <td>Writer. Total coffee scholar. Travel lover. Thinker. Troublemaker. Hardcore bacon practitioner.</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6163</th>\n      <td>Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>946 rows × 5 columns</p>\n</div>",
      "text/plain": "                                                                                                              Bios  \\\n7                           Certified web evangelist. Proud bacon trailblazer. Travel aficionado. Alcohol scholar.   \n9                    Hipster-friendly musicaholic. Wannabe tv fanatic. Certified gamer. Amateur coffee specialist.   \n10              Total entrepreneur. Proud web fanatic. Typical beer scholar. Student. Lifelong explorer. Tv maven.   \n11                                  Music fan. Beer geek. Web lover. Falls down a lot. Coffee nerd. Travel junkie.   \n12    Falls down a lot. Typical beer guru. Creator. Subtly charming alcohol enthusiast. Incurable tv buff. Reader.   \n...                                                                                                            ...   \n6133           Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.   \n6143               Writer. Total coffee scholar. Travel lover. Thinker. Troublemaker. Hardcore bacon practitioner.   \n6148           Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.   \n6158               Writer. Total coffee scholar. Travel lover. Thinker. Troublemaker. Hardcore bacon practitioner.   \n6163           Wannabe coffee practitioner. Troublemaker. Communicator. Friendly travel advocate. Reader. Thinker.   \n\n      Loation  Qualification  Specialisation  Cluster #  \n7         7.0            9.0             1.0          4  \n9         9.0            4.0             3.0          4  \n10        3.0            4.0             3.0          4  \n11        1.0            4.0             3.0          4  \n12        5.0            7.0             0.0          4  \n...       ...            ...             ...        ...  \n6133      0.0            7.0             6.0          4  \n6143      1.0            8.0             7.0          4  \n6148      1.0            8.0             9.0          4  \n6158      6.0            1.0             8.0          4  \n6163      7.0            7.0             3.0          4  \n\n[946 rows x 5 columns]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des_cluster = cluster_df[cluster_df['Cluster #']==designated_cluster[0]]\n",
    "\n",
    "des_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Top 10 Similar Profiles to our New Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n      <th>advocate</th>\n      <th>aficionado</th>\n      <th>alcohol</th>\n      <th>alcoholaholic</th>\n      <th>amateur</th>\n      <th>analyst</th>\n      <th>avid</th>\n      <th>...</th>\n      <th>tvaholic</th>\n      <th>twitter</th>\n      <th>typical</th>\n      <th>unapologetic</th>\n      <th>vechicle</th>\n      <th>wannabe</th>\n      <th>web</th>\n      <th>with</th>\n      <th>writer</th>\n      <th>zombie</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>5.0</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6143</th>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6148</th>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>9.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6158</th>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6163</th>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6165</th>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>947 rows × 98 columns</p>\n</div>",
      "text/plain": "      Loation  Qualification  Specialisation  advocate  aficionado  alcohol  \\\n7         7.0            9.0             1.0         0           1        1   \n9         9.0            4.0             3.0         0           0        0   \n10        3.0            4.0             3.0         0           0        0   \n11        1.0            4.0             3.0         0           0        0   \n12        5.0            7.0             0.0         0           0        1   \n...       ...            ...             ...       ...         ...      ...   \n6143      1.0            8.0             7.0         0           0        0   \n6148      1.0            8.0             9.0         1           0        0   \n6158      6.0            1.0             8.0         0           0        0   \n6163      7.0            7.0             3.0         1           0        0   \n6165      7.0            7.0             7.0         0           0        0   \n\n      alcoholaholic  amateur  analyst  avid  ...  tvaholic  twitter  typical  \\\n7                 0        0        0     0  ...         0        0        0   \n9                 0        1        0     0  ...         0        0        0   \n10                0        0        0     0  ...         0        0        1   \n11                0        0        0     0  ...         0        0        0   \n12                0        0        0     0  ...         0        0        1   \n...             ...      ...      ...   ...  ...       ...      ...      ...   \n6143              0        0        0     0  ...         0        0        0   \n6148              0        0        0     0  ...         0        0        0   \n6158              0        0        0     0  ...         0        0        0   \n6163              0        0        0     0  ...         0        0        0   \n6165              0        0        0     0  ...         0        0        0   \n\n      unapologetic  vechicle  wannabe  web  with  writer  zombie  \n7                0         0        0    1     0       0       0  \n9                0         0        1    0     0       0       0  \n10               0         0        0    1     0       0       0  \n11               0         0        0    1     0       0       0  \n12               0         0        0    0     0       0       0  \n...            ...       ...      ...  ...   ...     ...     ...  \n6143             0         0        0    0     0       1       0  \n6148             0         0        1    0     0       0       0  \n6158             0         0        0    0     0       1       0  \n6163             0         0        1    0     0       0       0  \n6165             0         1        0    0     1       0       0  \n\n[947 rows x 98 columns]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appending the new profile data\n",
    "des_cluster = des_cluster.append(new_profile, sort=False)\n",
    "\n",
    "# Fitting the vectorizer to the Bios\n",
    "cluster_x = vectorizer.fit_transform(des_cluster['Bios'])\n",
    "\n",
    "# Creating a new DF that contains the vectorized words\n",
    "cluster_v = pd.DataFrame(cluster_x.toarray(), index=des_cluster.index, columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Joining the Vectorized DF to the previous DF and dropping columns\n",
    "des_cluster = des_cluster.join(cluster_v).drop(['Bios', 'Cluster #'], axis=1)\n",
    "\n",
    "des_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations to find similar profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the Top 10 similar or correlated users to the new user\n",
    "user_n = new_profile.index[0]\n",
    "\n",
    "# Trasnposing the DF so that we are correlating with the index(users) and finding the correlation\n",
    "corr = des_cluster.T.corrwith(des_cluster.loc[user_n])\n",
    "\n",
    "# Creating a DF with the Top 10 most similar profiles\n",
    "top_10_sim = corr.sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 Similar profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bios</th>\n      <th>Loation</th>\n      <th>Qualification</th>\n      <th>Specialisation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3454</th>\n      <td>Coffee geek. Alcoholaholic. Extreme internet fan. Twitter lover.</td>\n      <td>9</td>\n      <td>8</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>5754</th>\n      <td>Total entrepreneur. Evil zombie enthusiast. Troublemaker. Avid food advocate.</td>\n      <td>9</td>\n      <td>9</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3875</th>\n      <td>General tv fanatic. Incurable bacon aficionado. Unapologetic beer trailblazer. Food junkie.</td>\n      <td>9</td>\n      <td>9</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4881</th>\n      <td>Avid explorer. Lifelong beer specialist. Incurable tv geek. Thinker. Communicator.</td>\n      <td>8</td>\n      <td>9</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2139</th>\n      <td>Coffee evangelist. Certified tv scholar. Web fanatic. Beer lover. Analyst.</td>\n      <td>8</td>\n      <td>9</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>5583</th>\n      <td>Passionate introvert. Internet specialist. Proud alcohol ninja. Typical food nerd. Entrepreneur.</td>\n      <td>9</td>\n      <td>9</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3935</th>\n      <td>General tv fanatic. Incurable bacon aficionado. Unapologetic beer trailblazer. Food junkie.</td>\n      <td>8</td>\n      <td>9</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>5193</th>\n      <td>Certified music trailblazer. Writer. Explorer. General entrepreneur. Zombie specialist.</td>\n      <td>8</td>\n      <td>8</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5133</th>\n      <td>Certified music trailblazer. Writer. Explorer. General entrepreneur. Zombie specialist.</td>\n      <td>8</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5223</th>\n      <td>Certified music trailblazer. Writer. Explorer. General entrepreneur. Zombie specialist.</td>\n      <td>8</td>\n      <td>7</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                                                                                  Bios  \\\n3454                                  Coffee geek. Alcoholaholic. Extreme internet fan. Twitter lover.   \n5754                     Total entrepreneur. Evil zombie enthusiast. Troublemaker. Avid food advocate.   \n3875       General tv fanatic. Incurable bacon aficionado. Unapologetic beer trailblazer. Food junkie.   \n4881                Avid explorer. Lifelong beer specialist. Incurable tv geek. Thinker. Communicator.   \n2139                        Coffee evangelist. Certified tv scholar. Web fanatic. Beer lover. Analyst.   \n5583  Passionate introvert. Internet specialist. Proud alcohol ninja. Typical food nerd. Entrepreneur.   \n3935       General tv fanatic. Incurable bacon aficionado. Unapologetic beer trailblazer. Food junkie.   \n5193           Certified music trailblazer. Writer. Explorer. General entrepreneur. Zombie specialist.   \n5133           Certified music trailblazer. Writer. Explorer. General entrepreneur. Zombie specialist.   \n5223           Certified music trailblazer. Writer. Explorer. General entrepreneur. Zombie specialist.   \n\n      Loation  Qualification  Specialisation  \n3454        9              8               8  \n5754        9              9               8  \n3875        9              9               9  \n4881        8              9               9  \n2139        8              9               8  \n5583        9              9               8  \n3935        8              9               9  \n5193        8              8               7  \n5133        8              7               7  \n5223        8              7               7  "
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.loc[top_10_sim.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Classification Model\n",
    "For future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['clf_model.joblib']"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(svm, \"clf_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conclusion on the Two Different Approaches\n",
    "The results for both approaches are the same.  The new profile ends up in the same cluster whether it is clustered or classified to be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}